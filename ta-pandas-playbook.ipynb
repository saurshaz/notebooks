{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Fine python notebookers >>>>>')\n",
    "\n",
    "\n",
    "NSE_COOKIE = 'AKA_A2=A; ak_bmsc=168F6F9C99D7910D5FBBBF25F2773ECB~000000000000000000000000000000~YAAQTjZ8aMHhWwh9AQAAOavRGA0yrYz6JL1S11fH8sf4mc9R3tMfTtWEXN4%2FZjrV1zv9ldq2pxOL4frY0bw3iMwhjVpK%2FKRemw9UiuI7Ig3YTVAyhlx%2FarWBAxxGnApmkfLIFUD4X95Et1uuxAgUmecWtbht16jawh6eUS7YNevGFHAYv%2B2XpAOSz2vq2iOMt3qX3E%2FY7AYhc%2Bo0PAYlCyo%2BEN1vXLDvHMyTyoWvUJizZHY4D8Ttn56CIem7GHTYnw1h6ntDP2Xl%2BRcFVRO62gZxWw%2BHSTpqET39nGn6ZhXnclgY1aBtj5xGMu0l1g2rvnF5UrN8CJ3Iz57B12gNQkakXcbjw4ZOXtYsaT4EcJbLB%2FfXiYRxTd8AoFDoaBaTj%2BliL04SP0GSh5rI'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymongo\n",
    "!pip install arctic\n",
    "from arctic import Arctic\n",
    "\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import talib as ta\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from pandas_datareader import data\n",
    "from watchlist import Watchlist # Is this failing? If so, copy it locally. See above.\n",
    "\n",
    "import time\n",
    "import os\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import shutil \n",
    "\n",
    "# ticker = \"20MICRONS\"\n",
    "home_dir = Path.home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# filtered_companies = [ 'MUTHOOTFIN.NS', 'MUTHOOTCAP.NS', 'COALINDIA.NS','ICICIPRULI.NS', 'DEEPAKNTR.NS', 'BAJAJELEC.NS', 'BAJAJ-AUTO.NS', 'EICHERMOT.NS', 'EASEMYTRIP.NS', 'BAJFINANCE.NS',  'DMART.NS', 'PERSISTENT.NS', 'IOB.NS', 'IOC.NS', 'IDFCFIRSTB.NS', 'MINDTREE.NS', 'BEL.NS', 'NIFTYBEES.NS', 'JKPAPER.NS', 'NAUKRI.NS', 'EASEMYTRIP.NS', '^NSEBANK', '^NSEI', 'BAJFINANCE.NS', 'BAJFINSV.NS', 'SBILIFE.NS', 'HDFCLIFE.NS',  'BRITANNIA.NS', 'VOLTAS.NS', 'CAMLINFINE.NS', 'INDHOTEL.NS', 'LEMONTREE.NS', 'GLAXO.NS', 'SUNPHARMA.NS', 'ASIANPAINT.NS', 'PRESTIGE.NS', 'DLF.NS', 'BERGERPAINTS.NS', 'POWERGRID.NS', 'ADANIPORTS.NS', 'ADANIENT.NS', 'DHAMPURSUG.NS', 'SRTRANSFIN.NS', 'GMBREW.NS', 'NUCLEUS.NS', 'CEATLTD.NS', 'ANDHRSUGAR.NS', 'PRAJIND.NS', 'IOC.NS', 'IGL.NS', 'INDIGO.NS', 'PVR.NS', 'LUPIN.NS', 'RBLBANK.NS', 'DEEPAKNITRITE.NS', 'FORCEMOTORS.NS', 'ASTRAL.NS', 'INDUSINDBK.NS', 'MFSL.NS', 'MRF.NS', 'BATAINDIA.NS', 'TATAPOWER.NS', 'MPHASIS.NS', 'SPICEJET.NS', 'INOXLEISUR' ,'NIFTY.NS', 'IBREALEST.NS', 'ASTERDM.NS',  'IBULHSGFIN.NS', 'BANKNIFTY.NS', 'NESTLEIND.NS', 'UBL.NS', 'MCDOWELLS.NS', 'LT.NS', 'LTI.NS', 'LTTS.NS', 'GRASIM.NS', 'RAYMOND.NS', 'ULTRATECHCEM.NS', 'ACC.NS', 'HAVELLS.NS', 'TATACOFFEE.NS', 'TATASTEEL.NS', 'TATAMOTORS.NS', 'ASHOKLEY.NS', 'PNB.NS', 'CANBK.NS', 'BANKBARODA.NS', 'IDFCFIRST.NS', 'TATACHEM.NS', 'HINDUNILVR.NS', 'GODREJPROP.NS', 'SOBHA.NS', 'APOLLOHOSP.NS', 'BEL.NS', 'FORTIS.NS', 'HINDCOPPER.NS', 'HINDZINC.NS',  'BHARTIARTL.NS', 'HCLTECH.NS', 'GAIL.NS', 'ONGC.NS', 'SAIL.NS', 'HINDALCO.NS', 'GLENMARK.NS', 'BPCL.NS', 'HDFCAMC.NS', 'NTPC.NS', 'ZEEL.NS', 'ZOMATO.NS', 'ICICIBANK.NS', 'SBIN.NS', 'IRCTC.NS', 'AXISBANK.NS',  'M&M.NS',  'ITC.NS', 'MARICO.NS', 'INDUSIND.NS', 'TITAN.NS', 'DIVISLAB.NS', 'PFIZER.NS', 'NESTLE.NS', 'PGHH.NS', 'TATACONSUM.NS', 'GODREJCP.NS', 'HAPPSTMNDS.NS', 'SONATASOFTW.NS', 'NAZARA.NS', 'BHEL.NS', 'COFORGE.NS', 'HDFC.NS', 'HDFCBANK.NS', 'KOTAKBANK.NS', 'CIPLA.NS', 'TCS.NS', 'CDSL.NS', 'BSE.NS', 'INFY.NS', 'WIPRO.NS', 'TECHM.NS', 'JSWSTEEL.NS', 'MANAPPURAM.NS', 'DRREDDY.NS']\n",
    "filtered_companies = [ 'MUTHOOTFIN.NS', 'MUTHOOTCAP.NS', 'COALINDIA.NS','ICICIPRULI.NS', 'DEEPAKNTR.NS', 'BAJAJELEC.NS', 'BAJAJ-AUTO.NS', 'EICHERMOT.NS', 'INFY.NS', 'WIPRO.NS', 'TECHM.NS', 'JSWSTEEL.NS', 'MANAPPURAM.NS', 'DRREDDY.NS' ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_oc(ticker):\n",
    "    # Let's get option chain for identified tickers and find the ideal strikes data\n",
    "    url = \"https://www.nseindia.com/api/option-chain-equities\"\n",
    "\n",
    "    querystring = {\"symbol\":ticker}\n",
    "\n",
    "    payload = \"\"\n",
    "    \n",
    "    headers = {'cookie': NSE_COOKIE}\n",
    "\n",
    "    response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring)\n",
    "    print(response.text)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "    \n",
    "# get_oc('ADANIPORTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns = ['open', 'high', 'low', 'close', 'volume', 'stock splits', 'aber_zg_5_15', 'aber_sg_5_15', 'aber_xg_5_15', 'aber_atr_5_15', 'accbl_20', 'accbm_20', 'accbu_20', 'ad', 'adosc_3_10', '    ', 'dmp_14', 'dmn_14', 'alma_10_6.0_0.85', 'amate_lr_8_21_2', 'amate_sr_8_21_2', 'ao_5_34', 'obv', 'obv_min_2', 'obv_max_2', 'obve_4', 'obve_12', 'aobv_lr_2', 'aobv_sr_2', 'apo_12_26', 'aroond_14', 'aroonu_14', 'aroonosc_14', 'atrr_14', 'bbl_5_2.0', 'bbm_5_2.0', 'bbu_5_2.0', 'bbb_5_2.0', 'bbp_5_2.0', 'bias_sma_26', 'bop', 'ar_26', 'br_26', 'cci_14_0.015', 'cdl_2crows', 'cdl_3blackcrows', 'cdl_3inside', 'cdl_3linestrike', 'cdl_3outside', 'cdl_3starsinsouth', 'cdl_3whitesoldiers', 'cdl_abandonedbaby', 'cdl_advanceblock', 'cdl_belthold', 'cdl_breakaway', 'cdl_closingmarubozu', 'cdl_concealbabyswall', 'cdl_counterattack', 'cdl_darkcloudcover', 'cdl_doji_10_0.1', 'cdl_dojistar', 'cdl_dragonflydoji', 'cdl_engulfing', 'cdl_eveningdojistar', 'cdl_eveningstar', 'cdl_gapsidesidewhite', 'cdl_gravestonedoji', 'cdl_hammer', 'cdl_hangingman', 'cdl_harami', 'cdl_haramicross', 'cdl_highwave', 'cdl_hikkake', 'cdl_hikkakemod', 'cdl_homingpigeon', 'cdl_identical3crows', 'cdl_inneck', 'cdl_inside', 'cdl_invertedhammer', 'cdl_kicking', 'cdl_kickingbylength', 'cdl_ladderbottom', 'cdl_longleggeddoji', 'cdl_longline', 'cdl_marubozu', 'cdl_matchinglow', 'cdl_mathold', 'cdl_morningdojistar', 'cdl_morningstar', 'cdl_onneck', 'cdl_piercing', 'cdl_rickshawman', 'cdl_risefall3methods', 'cdl_separatinglines', 'cdl_shootingstar', 'cdl_shortline', 'cdl_spinningtop', 'cdl_stalledpattern', 'cdl_sticksandwich', 'cdl_takuri', 'cdl_tasukigap', 'cdl_thrusting', 'cdl_tristar', 'cdl_unique3river', 'cdl_upsidegap2crows', 'cdl_xsidegap3methods', 'open_z_30_1', 'high_z_30_1', 'low_z_30_1', 'close_z_30_1', 'cfo_9', 'cg_10', 'chop_14_1_100', 'ckspl_10_3_20', 'cksps_10_3_20', 'cmf_20', 'cmo_14', 'copc_11_14_10', 'cti_12', 'ldecay_5', 'dec_1', 'dema_10', 'dcl_20_20', 'dcm_20_20', 'dcu_20_20', 'dpo_20', 'ebsw_40_10', 'efi_13', 'ema_10', 'entp_10', 'eom_14_100000000', 'er_10', 'bullp_13', 'bearp_13', 'fishert_9_1', 'fisherts_9_1', 'fwma_10', 'ha_open', 'ha_high', 'ha_low', 'ha_close', 'hilo_13_21', 'hilol_13_21', 'hilos_13_21', 'hl2', 'hlc3', 'hma_10', 'hwm', 'hwu', 'hwl', 'hwma_0.2_0.1_0.1', 'isa_9', 'isb_26', 'its_9', 'iks_26', 'ics_26', 'inc_1', 'inertia_20_14', 'jma_7_0', 'kama_10_2_30', 'kcle_20_2', 'kcbe_20_2', 'kcue_20_2', 'k_9_3', 'd_9_3', 'j_9_3', 'kst_10_15_20_30_10_10_10_15', 'ksts_9', 'kurt_30', 'kvo_34_55_13', 'kvos_34_55_13', 'lr_14', 'logret_1', 'macd_12_26_9', 'macdh_12_26_9', 'macds_12_26_9', 'mad_30', 'massi_9_25', 'mcgd_10', 'median_30', 'mfi_14', 'midpoint_2', 'midprice_2', 'mom_10', 'natr_14', 'nvi_1', 'ohlc4', 'pdist', 'pctret_1', 'pgo_14', 'ppo_12_26_9', 'ppoh_12_26_9', 'ppos_12_26_9', 'psarl_0.02_0.2', 'psars_0.02_0.2', 'psaraf_0.02_0.2', 'psarr_0.02_0.2', 'psl_12', 'pvi_1', 'pvo_12_26_9', 'pvoh_12_26_9', 'pvos_12_26_9', 'pvol', 'pvr', 'pvt', 'pwma_10', 'qqe_14_5_4.236', 'qqe_14_5_4.236_rsima', 'qqel_14_5_4.236', 'qqes_14_5_4.236', 'qs_10', 'qtl_30_0.5', 'rma_10', 'roc_10', 'rsi_14', 'rsx_14', 'rvgi_14_4', 'rvgis_14_4', 'rvi_14', 'sinwma_14', 'skew_30', 'slope_1', 'sma_10', 'smi_5_20_5', 'smis_5_20_5', 'smio_5_20_5', 'sqz_20_2.0_20_1.5', 'sqz_on', 'sqz_off', 'sqz_no', 'sqzpro_20_2.0_20_2_1.5_1', 'sqzpro_on_wide', 'sqzpro_on_normal', 'sqzpro_on_narrow', 'sqzpro_off', 'sqzpro_no', 'ssf_10_2', 'stc_10_12_26_0.5', 'stcmacd_10_12_26_0.5', 'stcstoch_10_12_26_0.5', 'stdev_30', 'stochk_14_3_3', 'stochd_14_3_3', 'stochrsik_14_14_3_3', 'stochrsid_14_14_3_3', 'supert_7_3.0', 'supertd_7_3.0', 'supertl_7_3.0', 'superts_7_3.0', 'swma_10', 't3_10_0.7', 'tema_10', 'thermo_20_2_0.5', 'thermoma_20_2_0.5', 'thermol_20_2_0.5', 'thermos_20_2_0.5', 'tos_stdevall_lr', 'tos_stdevall_l_1', 'tos_stdevall_u_1', 'tos_stdevall_l_2', 'tos_stdevall_u_2', 'tos_stdevall_l_3', 'tos_stdevall_u_3', 'trima_10', 'trix_30_9', 'trixs_30_9', 'truerange_1', 'tsi_13_25_13', 'tsis_13_25_13', 'ttm_trnd_6', 'ui_14', 'uo_7_14_28', 'var_30', 'vhf_28', 'vidya_14', 'vtxp_14', 'vtxm_14', 'vwap_d', 'vwma_10', 'wcp', 'willr_14', 'wma_10', 'zl_ema_10', 'zs_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arctic import Arctic\n",
    "\n",
    "# Connect to Local MONGODB\n",
    "store = Arctic('localhost')\n",
    "\n",
    "# library = store['NSEx']\n",
    "# library.delete({})\n",
    "\n",
    "# Create the library - defaults to VersionStore\n",
    "store.initialize_library('NSEx')\n",
    "# store.delete_library('NSEDATA')\n",
    "\n",
    "library = store['NSEx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def df_from_db(_ticker, tf):\n",
    "    try:\n",
    "        # _ticker = _ticker.replace('.NS', 'MANAPPURAM.NS_2d')\n",
    "        # print('libraries .. ', store.list_libraries())\n",
    "        print('fetching from local saved DB ticker data  .. ')\n",
    "        # Reading the data\n",
    "        symbol_df = library.read(_ticker + '_' + tf)\n",
    "        _ticker_data = symbol_df.data\n",
    "        print('fetched from local saved DB ticker data  .. ')\n",
    "        # _data =  list(library.query())\n",
    "        # print('ticker data length .. ', len(_ticker_data))\n",
    "        return _ticker_data\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "def recent_bars(df, tf: str = \"1d\"):\n",
    "    # All Data: 0, Last Four Years: 0.25, Last Two Years: 0.5, This Year: 1, Last Half Year: 2, Last Quarter: 4\n",
    "    yearly_divisor = {\"all\": 0, \"10y\": 0.1, \"5y\": 0.2, \"4y\": 0.25, \"3y\": 1./3, \"2y\": 0.5, \"1y\": 1, \"6mo\": 2, \"3mo\": 4, \"1mo\": 12, \"5d\": 73, \"1d\": 365, \"2d\": 365/2}\n",
    "    yd = yearly_divisor[tf] if tf in yearly_divisor.keys() else 0\n",
    "    return int(ta.RATE[\"TRADING_DAYS_PER_YEAR\"] / yd) if yd > 0 else df.shape[0]\n",
    "\n",
    "def fresh_load(_ticker, duration, tf: str = \"1d\"):\n",
    "    watch = Watchlist([_ticker], tf=tf, ds_name=\"yahoo\", timed=True)\n",
    "    data_source = \"yahoo\"\n",
    "    # If you have a Custom Strategy, you can use it here.\n",
    "    # watch.strategy = ta.CommonStrategy \n",
    "    watch.strategy = ta.AllStrategy\n",
    "    watch.load([_ticker], analyze=True, verbose=False)\n",
    "    asset = watch.data[_ticker]\n",
    "    recent = recent_bars(asset, duration)\n",
    "    asset.columns = asset.columns.str.lower()\n",
    "    asset.drop(columns=[\"dividends\", \"split\"], errors=\"ignore\", inplace=True)\n",
    "    asset = asset.copy().tail(recent)\n",
    "    # # Store the data in the library\n",
    "    library.write(_ticker + '_' + tf , asset, metadata={ 'source': 'custom', 'duration': duration})\n",
    "    return asset\n",
    "\n",
    "## compute all essential TA data points\n",
    "## method to compute all TAs using pandas-ta\n",
    "## TODO: the timeframe being used is 1min, there shall be an option to change that \n",
    "def get_technical_data(_ticker, duration, tf: str = '5min'):\n",
    "    print(' Processing ... ',_ticker)\n",
    "    df = df_from_db(_ticker, tf)\n",
    "    if df is not None:\n",
    "        # import pdb; pdb.set_trace()\n",
    "        if df is not None and len(df) > 0:\n",
    "            df.drop(columns=[\"dividends\", \"split\"], errors=\"ignore\", inplace=True)\n",
    "            return df\n",
    "        else:\n",
    "            return fresh_load(_ticker, duration, tf)\n",
    "    else:\n",
    "        return fresh_load(_ticker, duration, tf)\n",
    "\n",
    "    # # fetch option_chain (for NSE listed symbol)\n",
    "    # option_chain = get_oc(ticker)\n",
    "    # print(option_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# compute scores\n",
    "def compute_score(df, _s = 0):\n",
    "    if df['volume'] < 10000:\n",
    "        # LOW VOLUME, -20 from score\n",
    "     _s = _s - 10\n",
    "    if df['volume'] > 10000:\n",
    "        _s = _s + 10\n",
    "    if df['atrr_14']:\n",
    "        _s = _s + (df['atrr_14']/5) - 6\n",
    "    if df['close'] > df['wma_10']:\n",
    "        _s = _s + 10\n",
    "    if df['close'] < df['wma_10']:\n",
    "        _s = _s - 10\n",
    "    return _s\n",
    "\n",
    "df_scores = pd.DataFrame({})\n",
    "filter_criteria = {}\n",
    "\n",
    "# TODO: save and update all of this iterated data to a Mongo DB on each run of this logic (maybe to different tables)\n",
    "# TODO: create a DB query to filter the data on the basis of different filtering criterias and save scores for eachticker at different times\n",
    "# TODO: Simple select query for all data for a ticker for any duration () could help us in plotting the chart with all the tech indicators also (as we have them computed). build an API for same\n",
    "# TODO: make this data be able to run a flashback chart (having that running with echarts is also an option). this already exists\n",
    "# TODO: make backtests run on each ticker and have results saved for each duration in a parallel DB table\n",
    "for _ticker in filtered_companies:\n",
    "    df_scores['symbol'] = _ticker\n",
    "    df_5y = get_technical_data(_ticker, duration='5y', tf='1w')\n",
    "    df_1y = get_technical_data(_ticker, duration='1y', tf='1d')\n",
    "    df_1mo = get_technical_data(_ticker, duration='1mo', tf='1d')\n",
    "    df_5d = get_technical_data(_ticker, duration='5d', tf='1min')\n",
    "    df_2d = get_technical_data(_ticker, duration='3d', tf='1min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test of loading in rapid fashion from local DB (earlier saved copy)\n",
    "df_1d = get_technical_data('WIPRO.NS', duration='1y', tf='1d')\n",
    "print('df_1y ... ',df_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Empty DataFrame\n",
      "Columns: [ticker, scores_2d, scores_5d, scores_1mo, scores_1y, scores_5y]\n",
      "Index: []\n",
      " Processing ...  MUTHOOTFIN.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MUTHOOTFIN.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MUTHOOTFIN.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MUTHOOTFIN.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MUTHOOTFIN.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MUTHOOTCAP.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MUTHOOTCAP.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MUTHOOTCAP.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MUTHOOTCAP.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MUTHOOTCAP.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  COALINDIA.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  COALINDIA.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  COALINDIA.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  COALINDIA.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  COALINDIA.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  ICICIPRULI.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  ICICIPRULI.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  ICICIPRULI.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  ICICIPRULI.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  ICICIPRULI.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  DEEPAKNTR.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  DEEPAKNTR.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  DEEPAKNTR.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  DEEPAKNTR.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  DEEPAKNTR.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  BAJAJELEC.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  BAJAJELEC.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  BAJAJELEC.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  BAJAJELEC.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  BAJAJELEC.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  BAJAJ-AUTO.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  BAJAJ-AUTO.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  BAJAJ-AUTO.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  BAJAJ-AUTO.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  BAJAJ-AUTO.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  EICHERMOT.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  EICHERMOT.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  EICHERMOT.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  EICHERMOT.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  EICHERMOT.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  INFY.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  INFY.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  INFY.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  INFY.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  INFY.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  WIPRO.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  WIPRO.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  WIPRO.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  WIPRO.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  WIPRO.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  TECHM.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  TECHM.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  TECHM.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  TECHM.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  TECHM.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  JSWSTEEL.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  JSWSTEEL.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  JSWSTEEL.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  JSWSTEEL.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  JSWSTEEL.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MANAPPURAM.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MANAPPURAM.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MANAPPURAM.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MANAPPURAM.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  MANAPPURAM.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  DRREDDY.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  DRREDDY.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  DRREDDY.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  DRREDDY.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " Processing ...  DRREDDY.NS\n",
      "fetching from local saved DB ticker data  .. \n",
      "fetched from local saved DB ticker data  .. \n",
      " top scores ....                        ticker  scores_2d  scores_5d  scores_1mo  scores_1y  \\\n",
      "DRREDDY.NS        DRREDDY.NS  35.627569  35.627569   35.627569  35.627569   \n",
      "DEEPAKNTR.NS    DEEPAKNTR.NS  35.572180  35.572180   35.572180  35.572180   \n",
      "EICHERMOT.NS    EICHERMOT.NS  29.614644  29.614644   29.614644  29.614644   \n",
      "MUTHOOTFIN.NS  MUTHOOTFIN.NS  24.771313  24.771313   24.771313  24.771313   \n",
      "TECHM.NS            TECHM.NS  24.328202  24.328202   24.328202  24.328202   \n",
      "INFY.NS              INFY.NS  21.695208  21.695208   21.695208  21.695208   \n",
      "ICICIPRULI.NS  ICICIPRULI.NS  17.938469  17.938469   17.938469  17.938469   \n",
      "WIPRO.NS            WIPRO.NS  17.339624  17.339624   17.339624  17.339624   \n",
      "MANAPPURAM.NS  MANAPPURAM.NS  15.591921  15.591921   15.591921  15.591921   \n",
      "BAJAJ-AUTO.NS  BAJAJ-AUTO.NS  11.565127  11.565127   11.565127  11.565127   \n",
      "BAJAJELEC.NS    BAJAJELEC.NS   2.652648   2.652648    2.652648   2.652648   \n",
      "JSWSTEEL.NS      JSWSTEEL.NS  -1.978890  -1.978890   -1.978890  -1.978890   \n",
      "MUTHOOTCAP.NS  MUTHOOTCAP.NS  -4.007287  -4.007287   -4.007287  -4.007287   \n",
      "COALINDIA.NS    COALINDIA.NS  -4.899087  -4.899087   -4.899087  -4.899087   \n",
      "\n",
      "               scores_5y  \n",
      "DRREDDY.NS     35.627569  \n",
      "DEEPAKNTR.NS   35.572180  \n",
      "EICHERMOT.NS   29.614644  \n",
      "MUTHOOTFIN.NS  24.771313  \n",
      "TECHM.NS       24.328202  \n",
      "INFY.NS        21.695208  \n",
      "ICICIPRULI.NS  17.938469  \n",
      "WIPRO.NS       17.339624  \n",
      "MANAPPURAM.NS  15.591921  \n",
      "BAJAJ-AUTO.NS  11.565127  \n",
      "BAJAJELEC.NS    2.652648  \n",
      "JSWSTEEL.NS    -1.978890  \n",
      "MUTHOOTCAP.NS  -4.007287  \n",
      "COALINDIA.NS   -4.899087  \n"
     ]
    }
   ],
   "source": [
    "# get stored data from DB into a DF and do processing with it\n",
    "# df = df_from_db('BAJAJFINSV.NS', '1y')\n",
    "# print('1y data ... fetched df from db ', df.tail(1).to_json())\n",
    "\n",
    "# df_scores = pd.DataFrame([],columns=['ticker', 'scores_2d','scores_5d','scores_1mo','scores_1y','scores_5y'])\n",
    "\n",
    "\n",
    "df_scores = pd.DataFrame({'ticker': [], 'scores_2d': [], 'scores_5d': [],'scores_1mo': [],'scores_1y': [],'scores_5y': [] }, columns=['ticker', 'scores_2d','scores_5d','scores_1mo','scores_1y','scores_5y'])\n",
    "print(df_scores)\n",
    "# df_scores.reindex()\n",
    "for _ticker in filtered_companies:\n",
    "    df_5y = get_technical_data(_ticker, duration='5y', tf='1w')\n",
    "    df_1y = get_technical_data(_ticker, duration='1y', tf='1d')\n",
    "    df_1mo = get_technical_data(_ticker, duration='1mo', tf='1d')\n",
    "    df_5d = get_technical_data(_ticker, duration='5d', tf='5min')\n",
    "    df_2d = get_technical_data(_ticker, duration='3d', tf='5min')\n",
    "    \n",
    "    scores_5y = df_5y.tail(1).apply(lambda x: compute_score(x), axis=1)\n",
    "    scores_1y = df_1y.tail(1).apply(lambda x: compute_score(x), axis=1)\n",
    "    scores_1mo = df_1mo.tail(1).apply(lambda x: compute_score(x), axis=1)\n",
    "    scores_2d = df_2d.tail(1).apply(lambda x: compute_score(x), axis=1)\n",
    "    scores_5d = df_5d.tail(1).apply(lambda x: compute_score(x), axis=1)\n",
    "    df_scores.loc[_ticker] = [ _ticker, scores_2d.values[0], scores_5d.values[0], scores_1mo.values[0], scores_1y.values[0], scores_5y.values[0]]\n",
    "# Sort and get top most values\n",
    " \n",
    "top_scores = df_scores.sort_values(by=['scores_2d', 'scores_5d', 'scores_1mo', 'scores_1y', 'scores_5y'], ascending=False).head(15)\n",
    "print(' top scores .... ', top_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the saved data (different durations and time spans) upon different time intervals\n",
    "# 5y data - 1d wise tick :: upon each 1 day,\n",
    "    # adjust the Arctic version to be T1 + 1hr to T2 + 1hr(now) data and corresponding TA data\n",
    "# 1y data - 1d wise tick :: upon each 1 day,\n",
    "    # adjust the Arctic version to be T1 + 1hr to T2 + 1hr(now) data and corresponding TA data\n",
    "# 1mo data - 5min wise tick :: upon each 5 mins,\n",
    "    # adjust the Arctic version to be T1 + 15mins to T2 + 15mins(now) data and corresponding TA data\n",
    "# 5d data - min wise tick :: upon each 5 mins,\n",
    "    # adjust the Arctic version to be T1 + 5mins to T2 + 5mins(now) data and corresponding TA data\n",
    "# 2d data - min wise tick :: upon each 5 mins,\n",
    "    # adjust the Arctic version to be T1 + 5mins to T2 + 5mins(now) data and corresponding TA data\n",
    "# 1d data - min wise tick :: upon each 5 mins, \n",
    "    # adjust the Arctic version to be T1 + 5mins to T2 + 5mins(now) data and corresponding TA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Subhadeep Nandy's strategy (reversal by 55% + last candle atleast)\n",
    "## Trend detection\n",
    "## price is more than 13SMA\n",
    "\n",
    "# pullback detection\n",
    "# close of this is less than close earlier candle\n",
    "\n",
    "\n",
    "# BUY SIGNAL\n",
    "#                 (\n",
    "#                    price > 13sma (BULLISH)\n",
    "#                      and \n",
    "#                    price of this candle is less than last candle close (PULLBACK)\n",
    "#                     and\n",
    "#                    price is greater than OPEN + 55% of last candle range(last candle High - last candle Low)  (BREAKOUT)\n",
    "#                    \n",
    "#                 SL is OPEN at breakout\n",
    "# \n",
    "\n",
    "# SELL SIGNAL\n",
    "#                 (\n",
    "#                    price < 13sma (BEARISH)\n",
    "#                      and \n",
    "#                    CLOSE price of this candle is greater than last candle CLOSE (PULLUP)\n",
    "#                      and\n",
    "#                    price is less than CLOSE - 55% of last candle range(last candle High - last candle Low)  (BREAKDOWN)\n",
    " \n",
    "\n",
    "# MONEY MANAGEMENT\n",
    "# position sizing =    X (overall capital) divided by R (risk percentage)\n",
    "#\n",
    "# 500000 is the capital\n",
    "# \n",
    "# \n",
    "# 10000(2% of overall capital) / 37  = 270 shares\n",
    "# open of the candle when BUY trade is taken, \n",
    "# open of the candle when SELL trade is taken, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d787b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW WE START TO FETCH AND WRITE EACH TICKER's DATA TO APPROPRIATE DIRECTORY\n",
    "import talib as tb\n",
    "import datetime\n",
    "from pandas import DataFrame\n",
    "\n",
    "home_dir = Path.home()\n",
    "\n",
    "all_companies = pd.read_csv(\"~/NSE_tickers.csv\")['SYMBOLS'].values.tolist()\n",
    "scores_df = DataFrame({'company': [], '5m': [], '15m': [], '30m': [], '60m': [], '90m': []})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def paper_trades():\n",
    "    # top 40 rank trades\n",
    "    top_scores = read_scores\n",
    "    # TODO: add logic to place orders with entry, exit and sl price for 5 instruments\n",
    "    # write thes same orders to ~/orders.csv with one field 'status'\n",
    "    # based on price check (every time update_data is run), read ~/orders.csv and process changes if needed as per the price at that moment\n",
    "    # do not delete that row, just update the status\n",
    "    # have a PnL view with date, and net positions along with closed positions\n",
    "    # eventually this may go to DB, for now pull from orders.csv only\n",
    "    # long polling to be done every 10 mins (along with update_data)\n",
    "    # eventually this to be done on each tick change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - paper trading\n",
    "# - chartink like scanner\n",
    "# - charting with multiple indicators\n",
    "# - scheduled running of scan (right now on-demand)\n",
    "# - refine scoring mechanism for compite functions (refer to chartink scans)\n",
    "# - \n",
    "    \n",
    "def read_scores():\n",
    "    return pd.read_csv(f'{home_dir}/Companies/scores.csv').sort_values(['1d', '90m','60m','30m','15m','5m'],ascending = False).head(40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### FIXME: \n",
    "# - fix update_data logic, breaking right now, and error comes right now, only update data and run computation and get score shall be possible \n",
    "# - add more data to the score dataframe \n",
    "#       - (time of scan, price, pct change overall, pct change in each timeframe)\n",
    "#       - also shall be displayed along with current data, OI chain data as at that minute for that stock.\n",
    "#          This MAY help in identifying which strike shall be taken\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "# ### TODO:\n",
    "# ability to find prices of specified stock at any specified time of the day\n",
    "# ability to find option chain data of specified stock as per spot price and time for any specified time of the day\n",
    "# run these two things at every 10 mins (in addition to on-demand runnability) and update stock and options data (~/Companies/<companyName>/<CompanyName>NSE.csv and ~/Companies/<companyName>/<CompanyName>NFO.csv)\n",
    "\n",
    "\n",
    "# - run the scan at specified times (9:30, 11:00, 13:00, 14:00, 15:00 and 15:25 for now)\n",
    "#    - find top 5 stocks with MTF positive scores (all positives) :: BULLISH\n",
    "#    - find top 5 stocks with MTF negative scores (all negatives) :: BEARISH\n",
    "#    - find options with nearest strikes possible for BULLISH and BEARISH shortlists\n",
    "#       - get whatever price possible\n",
    "# - make papertrades in papertrades.csv (for date time basis 1 file per date, SL price and TARGET price specified)\n",
    "\n",
    "# - positions check (every n mins, in beginning every 10 mins). this can also be doing the score computation part in a consolidated manner\n",
    "# - run a loop every 10 mins for those trades (compare the CMP at that time with SL and TARGET prices, do the needful, stay open or close trades)\n",
    "#     - ability to find spot price and associated indicator metrics at any time needed\n",
    "#     - ability to find option strike price by stock and option price at any time specified needed\n",
    "#     - update the day's score in appropriate positions file\n",
    "\n",
    "# - present the date's Positions view with PnL based on positions_<date>.csv file in a positions dataframe \n",
    "#     - this shall be like zerodha positions view with net positions view (and closed positions also) \n",
    "\n",
    "# BACKTESTING:\n",
    "# Since we will have every 10 min data for entire day, we can backtest options and rate how good/nbad the scoring algo is\n",
    "\n",
    "\n",
    "# ### BELOW part is algo trading and may or maynot be of much usage\n",
    "# intraday indexes trade\n",
    "# purely based on OI data of indexes NIFTY and BN, at 9:20 make these trades   \n",
    "#     based on OI data, along with SL and TARGET for spread\n",
    "#     - long and short straddle \n",
    "#     - long and short strangle \n",
    "#     - calendar spread trade\n",
    "#     - iron condor\n",
    "#     - collar\n",
    "#     - call condor\n",
    "#      All these will be accompanied with SL and TARGET for and will be purely intraday\n",
    "\n",
    "# ### TO ADD later\n",
    "#     - covered call (buy stock and do shorting on options) \n",
    "#     - other features related to all strategies\n",
    "\n",
    "# be able to generate payoff chart for each\n",
    "# be able to have greeks for each strike along with OC (and make delta neutral strategies)\n",
    "# be able to have indicator data based target and stop losses in positions\n",
    "\n",
    "\n",
    "\n",
    "# Once all this makes sense (and profit) in Jupyter notebook\n",
    "# we can extract this to a mobile first system that these main capabilities as desired below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# What the system is not ?\n",
    "#  - A trading ideas platform (any social trading or tips based system which owns no responsibilities)\n",
    "#  - works great but lags on expiry days (when most needed)\n",
    "#  - broker bound system\n",
    "#  - opinionted/operator-driven system\n",
    "\n",
    "# What the system is aimed to become ?\n",
    "# A broker agnostic trading ecosystem (starting with screener, papertrading and strategies build and monitor)\n",
    "#  A set of widgets which are part of most popular trading apps, but are broker bound today\n",
    "#  - user interfaces are opinionated so the system is coupled of 2 parts\n",
    "#  1) - For the broker who wants the capability to add widgets in his existing site\n",
    "#            - ability to plug in paper trading widget\n",
    "#            - ability to integrate trade screener widget\n",
    "#            - ability to plug in strategy builder widget\n",
    "#            - ability to plug in alerting builder widget\n",
    "#            - ability to plug in charts data widget\n",
    "#            (example: A motilal oswal can show these capabilies by configuring the files as per instructions and display data in it's site)\n",
    "# \n",
    "#  2) - For retailer day trader \n",
    "#   An overall mobile first system (where retailer day trader can plug desired broker \n",
    "#                       if choosing to deploy to broker or choose to keep live trading separate)      \n",
    "# \n",
    "# \n",
    "\n",
    "# 1. suggested trades placement (algotrading with configurability of rules ... ala chartink)\n",
    "#       - lets us place papertrades/positions based on designed algo conditions\n",
    "#               - charting and oi data display later. right now trader knows it all\n",
    "#       - lets us take additional papertrades (ala neostox)\n",
    "#       - get alerted about possible trades (not take trades but have alerts coming to you ala chartink)\n",
    "#       - lets us watch positions (and place papertrades as per scores)\n",
    "\n",
    "# 2. Strategic Investment and strategy adjustments\n",
    "#       - have payoff graphs for placed trades (like sensibul and opstra)\n",
    "#       - have optionstrat like capability to design, deploy and monitor strategies\n",
    "#       - ability to backtest strategies (MTF basis)\n",
    "# \n",
    "# The system shall display indicative broker charges also as part of all PnL statements and info (minimize surprises)\n",
    "# system is aimed at becoming a broker agnostic portal (for brokers who want to become digital)\n",
    "# \n",
    "# release this app as \n",
    "#       - a browser extension UI (that imports/exports positions from/to you broker like zerodha, aliceblue etc)\n",
    "#       - a website \n",
    "#       - a mobile app (like optionstrat + chartink + neostox)\n",
    "# \n",
    "\n",
    "\n",
    "# Later features (NOT basic ones but on path towards becoming a broker agnostic trading system): \n",
    "#  - charting capability with multiple indicators (data already in prices and ta-lib)\n",
    "#  - top gainers and losers (again data from NSE)\n",
    "#  - implement tick by tick execution instead of every 10 mins\n",
    "#  - be able to scale to huge volumes (like expiry days) with no latency issues\n",
    "#  - option chain displayed \n",
    "#  - handle longer duration payoffs. in beginning all papertrades/positions to be squared off at EOD\n",
    "#  - display more technical data (like support, resitances, unusual volumes, related news etc)\n",
    "#  - more analytical data like OI study, automated alerts etc\n",
    "#  - fundamentals\n",
    "#  - Alerting and Rule based action mechanism (one that actually alerts and acts, when you set a rule)\n",
    "\n",
    "\n",
    "\n",
    "# REFERENCES: - \n",
    "# https://www.nseindia.com/market-data/real-time-data-subscription (NSE realtime data) 28 lacs per year for 1 min NSE and FNO per year\n",
    "# https://archives.nseindia.com/content/press/Snapshot_15_delayed_data.pdf (1.2 lacs per year for 15 min delayed data)\n",
    "# https://opstra.definedge.com/\n",
    "# https://in.tradingview.com/screener/\n",
    "# https://chartink.com/\n",
    "# https://neotrader.in/\n",
    "# https://neostox.com/\n",
    "# https://web.sensibull.com/\n",
    "# https://optionstrat.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python379jvsc74a57bd0066e8cbcad4da590cee410d50769781f2dd34998db2d5815197c4294647d3084"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}